{"cells":[{"source":"# Sowing Success: How Machine Learning Helps Farmers Select the Best Crops\n\n![Farmer in a field](farmer_in_a_field.jpg)\n\nMeasuring essential soil metrics such as nitrogen, phosphorous, potassium levels, and pH value is an important aspect of assessing soil condition. However, it can be an expensive and time-consuming process, which can cause farmers to prioritize which metrics to measure based on their budget constraints.\n\nFarmers have various options when it comes to deciding which crop to plant each season. Their primary objective is to maximize the yield of their crops, taking into account different factors. One crucial factor that affects crop growth is the condition of the soil in the field, which can be assessed by measuring basic elements such as nitrogen and potassium levels. Each crop has an ideal soil condition that ensures optimal growth and maximum yield.\n\nA farmer reached out to you as a machine learning expert for assistance in selecting the best crop for his field. They've provided you with a dataset called `soil_measures.csv`, which contains:\n\n- `\"N\"`: Nitrogen content ratio in the soil\n- `\"P\"`: Phosphorous content ratio in the soil\n- `\"K\"`: Potassium content ratio in the soil\n- `\"pH\"` value of the soil\n- `\"crop\"`: categorical values that contain various crops (target variable).\n\nEach row in this dataset represents various measures of the soil in a particular field. Based on these measurements, the crop specified in the `\"crop\"` column is the optimal choice for that field.  \n\nIn this project, you will build multi-class classification models to predict the type of `\"crop\"` and identify the single most importance feature for predictive performance.","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false}},"id":"d3d001b0-2e2f-4b58-8442-99520bad831f","cell_type":"markdown"},{"source":"# All required libraries are imported here for you.\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\n#Goal: Identify the single most importance feature for predictive performance for the selected crops.#\n\n# Load the dataset\ncrops = pd.read_csv(\"soil_measures.csv\")\ncrops\n\n# Check for missing values in Data\ncrops.isna().sum()\n\n# To get unique values for each column\nfor col in crops.columns:\n    print(f\"Unique values in {col}: {crops[col].unique()}\")\n\n# Features: all columns except \"crop\"\nfeatures = [col for col in crops.columns if col != \"crop\"]\n\n# Prepare X and y\nX = crops[features]\ny = crops[\"crop\"]\n\n# Split the data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Fit a logistic regression model\nmodel = LogisticRegression(max_iter=1000)\nmodel.fit(X_train, y_train)\n\n# Get feature importances (coefficients)\nimportances = pd.Series(abs(model.coef_[0]), index=features)\nmost_important_feature = importances.idxmax()\nprint(f\"The single most important feature is: {most_important_feature}\")\n\n# Print all feature importances\nprint(importances.sort_values(ascending=False))\n\n# Create a dictionary to store each feature's predictive performance\nfeatures_dict = {}\n\n# Evaluate predictive performance for each feature individually\nfor feature in features:\n    X_train_single = X_train[[feature]]\n    X_test_single = X_test[[feature]]\n# Specify multi_class='multinomial' \n    model_single = LogisticRegression(max_iter=1000, multi_class='multinomial', solver='lbfgs')\n    model_single.fit(X_train_single, y_train)\n    y_pred_single = model_single.predict(X_test_single)\n    accuracy = metrics.accuracy_score(y_test, y_pred_single)\n    features_dict[feature] = accuracy\n\nprint(\"Predictive performance (accuracy) for each feature:\")\nprint(features_dict)\n\nfeatures_dict = {}\n\nfor feature in ['N', 'P', 'K', 'ph']:\n    log_reg = LogisticRegression(multi_class='multinomial')\n    log_reg.fit(X_train[[feature]], y_train)\n    y_pred = log_reg.predict(X_test[[feature]])\n    f1_score = metrics.f1_score(y_test, y_pred, average='weighted')\n    features_dict[feature] = f1_score\n    print('F1 Score for {feature}: {f1_score}')\n\n#Best predictive feature variable\nbest_predictive_feature = {'F1 Score': features_dict}\nbest_predictive_feature = pd.DataFrame(best_predictive_feature).reset_index()\nbest_predictive_feature.columns = ['Variables', 'F1 Score']\nprint(f\"Best predictive feature is {best_predictive_feature.loc[0, 'Variables']} with {round(best_predictive_feature['F1 Score'].max(), 3)} F1 Score.\")\n\n#Best predictive feature is K with 0.239 F1 Score.\n\n\n","metadata":{"id":"bA5ajAmk7XH6","executionTime":37097,"lastSuccessfullyExecutedCode":"# All required libraries are imported here for you.\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\n#Goal: Identify the single most importance feature for predictive performance for the selected crops.#\n\n# Load the dataset\ncrops = pd.read_csv(\"soil_measures.csv\")\ncrops\n\n# Check for missing values in Data\ncrops.isna().sum()\n\n# To get unique values for each column\nfor col in crops.columns:\n    print(f\"Unique values in {col}: {crops[col].unique()}\")\n\n# Features: all columns except \"crop\"\nfeatures = [col for col in crops.columns if col != \"crop\"]\n\n# Prepare X and y\nX = crops[features]\ny = crops[\"crop\"]\n\n# Split the data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Fit a logistic regression model\nmodel = LogisticRegression(max_iter=1000)\nmodel.fit(X_train, y_train)\n\n# Get feature importances (coefficients)\nimportances = pd.Series(abs(model.coef_[0]), index=features)\nmost_important_feature = importances.idxmax()\nprint(f\"The single most important feature is: {most_important_feature}\")\n\n# Print all feature importances\nprint(importances.sort_values(ascending=False))\n\n# Create a dictionary to store each feature's predictive performance\nfeatures_dict = {}\n\n# Evaluate predictive performance for each feature individually\nfor feature in features:\n    X_train_single = X_train[[feature]]\n    X_test_single = X_test[[feature]]\n# Specify multi_class='multinomial' if needed, not by assignment\n    model_single = LogisticRegression(max_iter=1000, multi_class='multinomial', solver='lbfgs')\n    model_single.fit(X_train_single, y_train)\n    y_pred_single = model_single.predict(X_test_single)\n    accuracy = metrics.accuracy_score(y_test, y_pred_single)\n    features_dict[feature] = accuracy\n\nprint(\"Predictive performance (accuracy) for each feature:\")\nprint(features_dict)\n\nfeatures_dict = {}\n\nfor feature in ['N', 'P', 'K', 'ph']:\n    log_reg = LogisticRegression(multi_class='multinomial')\n    log_reg.fit(X_train[[feature]], y_train)\n    y_pred = log_reg.predict(X_test[[feature]])\n    f1_score = metrics.f1_score(y_test, y_pred, average='weighted')\n    features_dict[feature] = f1_score\n    print('F1 Score for {feature}: {f1_score}')\n\n#Best predictive feature variable\nbest_predictive_feature = {'F1 Score': features_dict}\nbest_predictive_feature = pd.DataFrame(best_predictive_feature).reset_index()\nbest_predictive_feature.columns = ['Variables', 'F1 Score']\nprint(f\"Best predictive feature is {best_predictive_feature.loc[0, 'Variables']} with {round(best_predictive_feature['F1 Score'].max(), 3)} F1 Score.\")\n\n\n\n","executionCancelledAt":null,"lastExecutedAt":1764541860765,"lastScheduledRunId":null,"lastExecutedByKernel":"1f98c9fd-be19-4109-abd6-1ce45cb2553f","outputsMetadata":{"0":{"height":616,"type":"stream"}},"visualizeDataframe":false,"version":"ag-charts-v1","collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false}},"id":"d0eb4f16-5a99-460d-a5ba-706b7ef0bbe7","cell_type":"code","execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":"Unique values in N: [ 90  85  60  74  78  69  94  89  68  91  93  77  88  76  67  83  98  66\n  97  84  73  92  95  99  63  62  64  82  79  65  75  71  72  70  86  61\n  81  80 100  87  96  40  23  39  22  36  32  58  59  42  28  43  27  50\n  25  31  26  54  57  49  46  38  35  52  44  24  29  20  56  37  51  41\n  34  30  33  47  53  45  48  13   2  17  12   6  10  19  11  18  21  16\n   9   1   7   8   0   3   4   5  14  15  55 105 108 118 101 106 109 117\n 114 110 112 111 102 116 119 107 104 103 120 113 115 133 136 126 121 129\n 122 140 131 135 123 125 139 132 127 130 134]\nUnique values in P: [ 42  58  55  35  37  53  54  46  56  50  48  38  45  40  59  41  47  49\n  51  57  39  43  44  60  52  36  72  67  73  70  62  74  66  63  71  78\n  80  68  65  77  76  79  61  64  69  75  24  18  26  27  25  21  30  11\n   5  10   7  20  22  15  23   8  16  29  17   6  19  13   9  14  28  94\n  95  92  89  88  87  85  86  83  91  81  84  90  82  93  33  31  34  32\n 130 144 123 125 131 140 122 134 145 139 141 138 136 132 133 121 126 120\n 142 135 129 128 137 127 124 143  12]\nUnique values in K: [ 43  41  44  40  42  38  36  37  39  35  45  16  17  21  20  19  25  22\n  15  18  23  24  77  84  85  81  75  79  76  83  78  80  82  46  50  53\n  54  49  55  52  47  48  51  27  31  32  34  33  30  28  29  26 195 204\n 205 196 198 197 203 201 202 199 200  12  13   6   9  10  14   8   7   5\n  11]\nUnique values in ph: [6.50298529 7.03809636 7.84020714 ... 6.36260785 6.75879255 6.77983261]\nUnique values in crop: ['rice' 'maize' 'chickpea' 'kidneybeans' 'pigeonpeas' 'mothbeans'\n 'mungbean' 'blackgram' 'lentil' 'pomegranate' 'banana' 'mango' 'grapes'\n 'watermelon' 'muskmelon' 'apple' 'orange' 'papaya' 'coconut' 'cotton'\n 'jute' 'coffee']\nThe single most important feature is: ph\nph    2.602262\nK     1.112025\nN     0.524261\nP     0.168258\ndtype: float64\nPredictive performance (accuracy) for each feature:\n{'N': 0.1431818181818182, 'P': 0.18863636363636363, 'K': 0.25681818181818183, 'ph': 0.09772727272727273}\nF1 Score for {feature}: {f1_score}\nF1 Score for {feature}: {f1_score}\nF1 Score for {feature}: {f1_score}\nF1 Score for {feature}: {f1_score}\nBest predictive feature is K with 0.239 F1 Score.\n"}]}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}